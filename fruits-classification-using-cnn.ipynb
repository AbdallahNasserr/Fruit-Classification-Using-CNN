{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2609027,"sourceType":"datasetVersion","datasetId":5857}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 20px; border-radius: 5px; text-align: center; border: 4px solid white;\">\n    <h1 style=\"color: white; font-size: 30px; font-weight: bold;\">Fruit Classification</h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Deloyment using Streamlit: [Link](https://www.kaggle.com/code/abdallahprogrammer/fruit-classification-using-cnn-deployment)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Importing Libraries & Dataset</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os        \nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport glob as gb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\nimport matplotlib.pyplot as plt\nimport PIL\nimport cv2\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom keras.layers import Dropout","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-05-17T01:02:27.375704Z","iopub.execute_input":"2024-05-17T01:02:27.376301Z","iopub.status.idle":"2024-05-17T01:02:39.549625Z","shell.execute_reply.started":"2024-05-17T01:02:27.376273Z","shell.execute_reply":"2024-05-17T01:02:39.548535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Generating DataFrame</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"base_dir = '/kaggle/input/fruits-360_dataset/fruits-360'\ntrain_dir = os.path.join(base_dir,  'Training')\ntest_dir = os.path.join(base_dir, 'Test')\nfor dirpath, dirnames, filenames in os.walk(train_dir):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:02:39.551538Z","iopub.execute_input":"2024-05-17T01:02:39.552193Z","iopub.status.idle":"2024-05-17T01:02:59.412248Z","shell.execute_reply.started":"2024-05-17T01:02:39.552161Z","shell.execute_reply":"2024-05-17T01:02:59.411315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Base directory path on Kaggle\nbase_dir = '/kaggle/input/fruits-360_dataset/fruits-360'\ntrain_dir = os.path.join(base_dir, 'Training')\nvalidation_dir = os.path.join(base_dir, 'Test')\n\ndef create_dataframe(data_path):\n    df = []\n    for c in os.listdir(data_path):\n        class_folder = os.path.join(data_path, c)\n        for f in os.listdir(class_folder):\n            f_path = os.path.join(class_folder, f)\n            if f_path.endswith('jpg'):\n                df.append([f_path, c])\n    return pd.DataFrame(df, columns=('filename', 'class'))\n\n# Get the sorted list of classes\nclasses = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n\n# Create dataframes\ndf_train = create_dataframe(train_dir)\ndf_test = create_dataframe(validation_dir)\n\n# Display the dataframes\ndf_train, df_test\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:02:59.413635Z","iopub.execute_input":"2024-05-17T01:02:59.413999Z","iopub.status.idle":"2024-05-17T01:03:09.098782Z","shell.execute_reply.started":"2024-05-17T01:02:59.413968Z","shell.execute_reply":"2024-05-17T01:03:09.097871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:03:09.100848Z","iopub.execute_input":"2024-05-17T01:03:09.101199Z","iopub.status.idle":"2024-05-17T01:03:09.111410Z","shell.execute_reply.started":"2024-05-17T01:03:09.101167Z","shell.execute_reply":"2024-05-17T01:03:09.110281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Splitting Training file into Train and Validation</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"df_train, df_val = train_test_split(df_train, test_size=0.30, random_state=0)\ndf_train,df_val","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:03:14.611323Z","iopub.execute_input":"2024-05-17T01:03:14.611976Z","iopub.status.idle":"2024-05-17T01:03:14.632637Z","shell.execute_reply.started":"2024-05-17T01:03:14.611947Z","shell.execute_reply":"2024-05-17T01:03:14.631663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Eploratory Data Analysis (EDA)</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Function to count the number of images in a directory\ndef count_images_in_directory(directory):\n    total_images = 0\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.jpg') or file.endswith('.png'):  # Adjust file extensions as needed\n                total_images += 1\n    return total_images\n\n# Count the number of images in the train and test directories\ntrain_image_count = count_images_in_directory(train_dir)\ntest_image_count = count_images_in_directory(test_dir)\n\nprint(\"Number of images in train directory:\", train_image_count)\nprint(\"Number of images in test directory:\", test_image_count)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:03:20.676323Z","iopub.execute_input":"2024-05-17T01:03:20.676936Z","iopub.status.idle":"2024-05-17T01:03:20.949083Z","shell.execute_reply.started":"2024-05-17T01:03:20.676904Z","shell.execute_reply":"2024-05-17T01:03:20.948023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pathlib\ndata_dir = pathlib.Path(train_dir) \nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')])) \nprint(class_names)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:03:23.897090Z","iopub.execute_input":"2024-05-17T01:03:23.897693Z","iopub.status.idle":"2024-05-17T01:03:23.904919Z","shell.execute_reply.started":"2024-05-17T01:03:23.897662Z","shell.execute_reply":"2024-05-17T01:03:23.903954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fruits = list(data_dir.glob('Apple Golden 1/*.jpg'))\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = PIL.Image.open(str(fruits[i]))\n    plt.imshow(img)\n    plt.axis('off')\n\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:03:25.179233Z","iopub.execute_input":"2024-05-17T01:03:25.179608Z","iopub.status.idle":"2024-05-17T01:03:25.855346Z","shell.execute_reply.started":"2024-05-17T01:03:25.179580Z","shell.execute_reply":"2024-05-17T01:03:25.854423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Function to count the number of images in each class\ndef count_images_per_class(directory):\n    class_counts = {}\n    for class_name in os.listdir(directory):\n        class_path = os.path.join(directory, class_name)\n        if os.path.isdir(class_path):\n            class_counts[class_name] = len(os.listdir(class_path))\n    return class_counts\n\n# Count the number of images per class in the training directory\ntrain_class_counts = count_images_per_class(data_dir)\n\n# Sort class counts by their values\nsorted_class_counts = dict(sorted(train_class_counts.items(), key=lambda item: item[1]))\n\n# Take the top 10 and bottom 10 classes\ntop_10_classes = dict(list(sorted_class_counts.items())[-10:])\nbottom_10_classes = dict(list(sorted_class_counts.items())[:10])\n\n# Plot the top 10 classes\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.bar(top_10_classes.keys(), top_10_classes.values(), color='skyblue')\nplt.title('Top 10 Class Distribution')\nplt.xlabel('Class Labels')\nplt.ylabel('Number of Images')\nplt.xticks(rotation=90)  # Rotate x-axis labels vertically\n\n# Plot the bottom 10 classes\nplt.subplot(1, 2, 2)\nplt.bar(bottom_10_classes.keys(), bottom_10_classes.values(), color='skyblue')\nplt.title('Bottom 10 Class Distribution')\nplt.xlabel('Class Labels')\nplt.ylabel('Number of Images')\nplt.xticks(rotation=90)  # Rotate x-axis labels vertically\n\nplt.tight_layout()\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:03:30.357184Z","iopub.execute_input":"2024-05-17T01:03:30.357799Z","iopub.status.idle":"2024-05-17T01:03:31.011508Z","shell.execute_reply.started":"2024-05-17T01:03:30.357765Z","shell.execute_reply":"2024-05-17T01:03:31.010693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Select 3 random image paths from the directory\nselected_image_paths = np.random.choice(list(data_dir.glob(\"Apple Braeburn/*.jpg\")), 3, replace=False)\n\n# Iterate over the selected image paths\nfor image_path in selected_image_paths:\n    # Load an image\n    image = cv2.imread(str(image_path))\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Calculate histogram\n    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n    # Plot histogram\n    plt.figure()\n    plt.title('Histogram - {}'.format(image_path.name))\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    plt.plot(hist)\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:03:35.524685Z","iopub.execute_input":"2024-05-17T01:03:35.525486Z","iopub.status.idle":"2024-05-17T01:03:36.536633Z","shell.execute_reply.started":"2024-05-17T01:03:35.525444Z","shell.execute_reply":"2024-05-17T01:03:36.535688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport glob as gb\n# Define the data directory\n# List to store image sizes\nsize = []\n# Iterate through each folder in the seg_train directory\nfor folder in os.listdir(os.path.join(data_dir)):\n    # Glob all jpg files in the current folder\n    files = gb.glob(os.path.join(data_dir, folder, '*.jpg'))\n    # Iterate through each file\n    for file in files:\n        # Read the image and get its shape\n        image = plt.imread(file)\n        size.append(image.shape)\n# Count the occurrence of each size\nsize_counts = pd.Series(size).value_counts()\n# Display the counts of each size\nprint(size_counts)\n# Print the most common size\nmost_common_size = size_counts.idxmax()\nprint(\"Most common size:\", most_common_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def view_random_image(target_dir, target_class):\n    # Setup target directory (we'll view images from here)\n    target_folder = os.path.join(target_dir, target_class)\n\n    # Get a random image path\n    random_image = random.choice(os.listdir(target_folder))\n\n    # Read in the image\n    img = mpimg.imread(os.path.join(target_folder, random_image))\n\n    return img\n\n# Define the number of rows and columns for subplots\nrows = 17  # Adjust according to the total number of classes (131 in this case)\ncols = 8   # Adjust the number of columns as needed","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:03:40.912593Z","iopub.execute_input":"2024-05-17T01:03:40.913749Z","iopub.status.idle":"2024-05-17T01:03:40.919325Z","shell.execute_reply.started":"2024-05-17T01:03:40.913707Z","shell.execute_reply":"2024-05-17T01:03:40.918363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n# View a random image from the training dataset for all classes\nfor i, class_name in enumerate(class_names):\n    img = view_random_image(target_dir=data_dir, target_class=class_name)\n    plt.subplot(rows, cols, i + 1)\n    plt.imshow(img)\n    plt.title(class_name)\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:04:23.849137Z","iopub.execute_input":"2024-05-17T01:04:23.849965Z","iopub.status.idle":"2024-05-17T01:04:33.767089Z","shell.execute_reply.started":"2024-05-17T01:04:23.849929Z","shell.execute_reply":"2024-05-17T01:04:33.766117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Preprocessing & Using ImageDataGenerator() to Read Images</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"train_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n    df_train,\n    target_size=(100, 100),\n    batch_size=60,\n    classes=classes,\n    class_mode='categorical'  \n)\nval_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n    df_val,\n    target_size=(100, 100),\n    batch_size=75,\n    classes=classes,\n    class_mode='categorical',\n    shuffle=False\n)\ntest_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n    df_test,\n    target_size=(100, 100),\n    batch_size=75,\n    classes=classes,\n    class_mode='categorical',\n    shuffle=False\n)\ntrain_generator = train_gen\nvalidation_generator = val_gen\ntest_generator = test_gen","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:04:51.475163Z","iopub.execute_input":"2024-05-17T01:04:51.475868Z","iopub.status.idle":"2024-05-17T01:05:34.495226Z","shell.execute_reply.started":"2024-05-17T01:04:51.475838Z","shell.execute_reply":"2024-05-17T01:05:34.494466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_gen.classes)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:05:34.496620Z","iopub.execute_input":"2024-05-17T01:05:34.496907Z","iopub.status.idle":"2024-05-17T01:05:34.520712Z","shell.execute_reply.started":"2024-05-17T01:05:34.496883Z","shell.execute_reply":"2024-05-17T01:05:34.519859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = test_generator.classes\ny_test","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:05:37.367314Z","iopub.execute_input":"2024-05-17T01:05:37.367673Z","iopub.status.idle":"2024-05-17T01:05:37.390510Z","shell.execute_reply.started":"2024-05-17T01:05:37.367645Z","shell.execute_reply":"2024-05-17T01:05:37.389606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen.class_indices","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:05:42.665078Z","iopub.execute_input":"2024-05-17T01:05:42.665451Z","iopub.status.idle":"2024-05-17T01:05:42.676250Z","shell.execute_reply.started":"2024-05-17T01:05:42.665422Z","shell.execute_reply":"2024-05-17T01:05:42.675279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Modeling</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(512, activation='relu'),\n    keras.layers.Dropout(rate=0.5) ,            \n    Dense(131, activation='softmax')  \n])","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:05:46.161132Z","iopub.execute_input":"2024-05-17T01:05:46.161812Z","iopub.status.idle":"2024-05-17T01:05:46.907924Z","shell.execute_reply.started":"2024-05-17T01:05:46.161781Z","shell.execute_reply":"2024-05-17T01:05:46.907143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.metrics import Accuracy, Precision, Recall","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:05:56.256715Z","iopub.execute_input":"2024-05-17T01:05:56.257063Z","iopub.status.idle":"2024-05-17T01:05:56.263449Z","shell.execute_reply.started":"2024-05-17T01:05:56.257036Z","shell.execute_reply":"2024-05-17T01:05:56.262508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\n# Define F1 Score metric\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(name=name, **kwargs)\n        self.precision = Precision()\n        self.recall = Recall()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        precision = self.precision.result()\n        recall = self.recall.result()\n        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n    def reset_state(self):  # Renamed from reset_states\n        self.precision.reset_state()\n        self.recall.reset_state()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:05:57.348992Z","iopub.execute_input":"2024-05-17T01:05:57.349763Z","iopub.status.idle":"2024-05-17T01:05:57.357787Z","shell.execute_reply.started":"2024-05-17T01:05:57.349732Z","shell.execute_reply":"2024-05-17T01:05:57.356753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = [\n    'accuracy',\n    Precision(),  # Precision\n    Recall(),     # Recall\n    F1Score()\n]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:05:58.465736Z","iopub.execute_input":"2024-05-17T01:05:58.466092Z","iopub.status.idle":"2024-05-17T01:05:58.485588Z","shell.execute_reply.started":"2024-05-17T01:05:58.466065Z","shell.execute_reply":"2024-05-17T01:05:58.484614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',  \n              metrics=metrics)\n\nmodel.fit(\n    train_generator,\n    steps_per_epoch=8,\n    epochs=55,\n    verbose=1,\n    validation_data=validation_generator,\n    validation_steps=8\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:05:59.538595Z","iopub.execute_input":"2024-05-17T01:05:59.538956Z","iopub.status.idle":"2024-05-17T01:09:34.473535Z","shell.execute_reply.started":"2024-05-17T01:05:59.538929Z","shell.execute_reply":"2024-05-17T01:09:34.472519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model2.h5')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:10:14.557468Z","iopub.execute_input":"2024-05-17T01:10:14.558356Z","iopub.status.idle":"2024-05-17T01:10:14.752191Z","shell.execute_reply.started":"2024-05-17T01:10:14.558327Z","shell.execute_reply":"2024-05-17T01:10:14.751357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result = model.evaluate(test_generator)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:10:15.629089Z","iopub.execute_input":"2024-05-17T01:10:15.629447Z","iopub.status.idle":"2024-05-17T01:13:01.560316Z","shell.execute_reply.started":"2024-05-17T01:10:15.629420Z","shell.execute_reply":"2024-05-17T01:13:01.559456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test Accuracy:\", eval_result[1] )\nprint(\"Test Loss:\", eval_result[0] )\nprint(\"Test Precision:\", eval_result[2] )\nprint(\"Test Recall:\", eval_result[1] )\nprint(\"Test f1_score:\", eval_result[1] )","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:14:04.084979Z","iopub.execute_input":"2024-05-17T01:14:04.085678Z","iopub.status.idle":"2024-05-17T01:14:04.097231Z","shell.execute_reply.started":"2024-05-17T01:14:04.085645Z","shell.execute_reply":"2024-05-17T01:14:04.096329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\n\ndef predict_image(img_path, model):\n    fruits = train_generator.class_indices\n    img = image.load_img(img_path, target_size=(100, 100))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)  \n    img_array /= 255.0  \n\n    prediction = model.predict(img_array)\n    predicted_class = np.argmax(prediction)\n    predicted_label = [k for k, v in fruits.items() if v == predicted_class][0]\n    \n    print(\"Prediccion:\", predicted_label)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()\npredict_image('/kaggle/input/fruits-360_dataset/fruits-360/Test/Ginger Root/128_100.jpg', model)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:16:58.286505Z","iopub.execute_input":"2024-05-17T01:16:58.287266Z","iopub.status.idle":"2024-05-17T01:16:58.409723Z","shell.execute_reply.started":"2024-05-17T01:16:58.287237Z","shell.execute_reply":"2024-05-17T01:16:58.408540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">GRAD-CAM</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def grad_cam_heatmap(image, last_conv_layer_name='last_conv'):\n    \n    if model.layers[0].__class__.__name__ == 'Functional':\n        last_conv_layer_idx = 0\n        last_conv_layer_model = model.layers[0]\n    else:\n        last_conv_layer = model.get_layer(last_conv_layer_name)\n        last_conv_layer_idx = model.layers.index(last_conv_layer)\n        last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n\n\n    classifier_input = tf.keras.Input(shape=last_conv_layer_model.output.shape[1:])\n    x = classifier_input\n    classifier_layers = model.layers[last_conv_layer_idx+1:]\n    for layer in classifier_layers:\n        x = layer(x)\n    classifier_model = tf.keras.Model(classifier_input, x)\n\n\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(image)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n        #print(CLASSES[top_pred_index])\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n\n    return heatmap","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:22:11.250730Z","iopub.execute_input":"2024-05-17T01:22:11.251647Z","iopub.status.idle":"2024-05-17T01:22:11.262729Z","shell.execute_reply.started":"2024-05-17T01:22:11.251597Z","shell.execute_reply":"2024-05-17T01:22:11.261710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef grad_cam_heatmap(img_path, model, last_conv_layer_name='last_conv'):\n    img = cv2.imread(img_path)\n    img = np.expand_dims(img, axis=0).astype('float32') / 255.0\n\n    if model.layers[0].__class__.__name__ == 'Functional':\n        last_conv_layer_idx = 0\n        last_conv_layer_model = model.layers[0]\n    else:\n        last_conv_layer = model.get_layer(last_conv_layer_name)\n        last_conv_layer_idx = model.layers.index(last_conv_layer)\n        last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n\n    classifier_input = tf.keras.Input(shape=last_conv_layer_model.output.shape[1:])\n    x = classifier_input\n    classifier_layers = model.layers[last_conv_layer_idx+1:]\n    for layer in classifier_layers:\n        x = layer(x)\n    classifier_model = tf.keras.Model(classifier_input, x)\n\n    with tf.GradientTape() as tape:\n        last_conv_layer_output = last_conv_layer_model(img)\n        tape.watch(last_conv_layer_output)\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n\n    return heatmap\n\ndef grad_cam(img_path, model, last_conv_layer_name='conv2d_2'):\n    colors = plt.cm.jet(np.arange(256))[:, :3]\n    gc_mask = grad_cam_heatmap(img_path, model, last_conv_layer_name)\n    gc_mask_uint8 = (gc_mask*255.0).astype('uint8')\n    heatmap = colors[gc_mask_uint8]\n    img = cv2.imread(img_path).astype('uint8')\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = (heatmap*255).astype('uint8')\n    img_overlay = cv2.addWeighted(src1=img, alpha=0.6, src2=heatmap, beta=0.4, gamma=0.0)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cv2.cvtColor(img_overlay, cv2.COLOR_BGR2RGB))\n\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.set_label('Activation')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:22:15.460410Z","iopub.execute_input":"2024-05-17T01:22:15.461248Z","iopub.status.idle":"2024-05-17T01:22:15.477899Z","shell.execute_reply.started":"2024-05-17T01:22:15.461218Z","shell.execute_reply":"2024-05-17T01:22:15.476958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad_cam(img,model)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:22:19.246905Z","iopub.execute_input":"2024-05-17T01:22:19.247739Z","iopub.status.idle":"2024-05-17T01:22:19.610367Z","shell.execute_reply.started":"2024-05-17T01:22:19.247708Z","shell.execute_reply":"2024-05-17T01:22:19.609449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:17:02.847976Z","iopub.execute_input":"2024-05-17T01:17:02.848333Z","iopub.status.idle":"2024-05-17T01:17:02.875321Z","shell.execute_reply.started":"2024-05-17T01:17:02.848306Z","shell.execute_reply":"2024-05-17T01:17:02.874489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-image: linear-gradient(to right, #6a11cb 0%, #2575fc 100%); padding: 10px; border-radius: 3px; text-align: center; border: 2px solid white;\">\n    <h1 style=\"color: white; font-size: 20px; font-weight: bold;\">Confusion Matrix and Performance Measurements</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    precision = Precision()(y_true, y_pred)\n    recall = Recall()(y_true, y_pred)\n    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:17:06.789127Z","iopub.execute_input":"2024-05-17T01:17:06.789485Z","iopub.status.idle":"2024-05-17T01:17:06.795075Z","shell.execute_reply.started":"2024-05-17T01:17:06.789444Z","shell.execute_reply":"2024-05-17T01:17:06.794158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_generator)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:17:09.758155Z","iopub.execute_input":"2024-05-17T01:17:09.758523Z","iopub.status.idle":"2024-05-17T01:17:36.734977Z","shell.execute_reply.started":"2024-05-17T01:17:09.758491Z","shell.execute_reply":"2024-05-17T01:17:36.733995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.argmax(y_pred, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:17:42.389463Z","iopub.execute_input":"2024-05-17T01:17:42.390372Z","iopub.status.idle":"2024-05-17T01:17:42.396698Z","shell.execute_reply.started":"2024-05-17T01:17:42.390332Z","shell.execute_reply":"2024-05-17T01:17:42.395747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-17T01:17:43.713191Z","iopub.execute_input":"2024-05-17T01:17:43.713588Z","iopub.status.idle":"2024-05-17T01:17:43.720250Z","shell.execute_reply.started":"2024-05-17T01:17:43.713558Z","shell.execute_reply":"2024-05-17T01:17:43.719318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\n# Construct confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Extract TP, FP, TN, FN\nTP = cm[1, 1]\nFP = cm[0, 1]\nTN = cm[0, 0]\nFN = cm[1, 0]\n\n# Compute metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# Print TP, FP, TN, FN\nprint(\"True Positives (TP):\", TP)\nprint(\"False Positives (FP):\", FP)\nprint(\"True Negatives (TN):\", TN)\nprint(\"False Negatives (FN):\", FN)\n\n# Print Confusion Matrix\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n\n# Plot Metrics\nmetrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\nmetrics_values = [accuracy, precision, recall, f1]\n\nplt.figure(figsize=(10, 6))\nbarplot = sns.barplot(x=metrics_values, y=metrics_names, palette=\"viridis\")\nplt.xlabel('Metric Value')\nplt.title('Performance Metrics')\nplt.xlim(0, 1)  # Adjust the x-axis limits if needed\n\n# Add data labels on each bar\nfor i, v in enumerate(metrics_values):\n    barplot.text(v + 0.01, i, str(round(v, 2)), color='black', ha='left')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:17:45.438248Z","iopub.execute_input":"2024-05-17T01:17:45.439114Z","iopub.status.idle":"2024-05-17T01:18:14.031288Z","shell.execute_reply.started":"2024-05-17T01:17:45.439081Z","shell.execute_reply":"2024-05-17T01:18:14.030382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have the true labels (y_test) and predicted labels (y_pred)\n# Assuming you already have the confusion matrix (cm)\n# Assuming you have a list of class names (class_names)\n\n# Calculate the frequency of each class in the true labels\nclass_freq = {class_label: sum([1 for label in y_test if label == class_label]) for class_label in set(y_test)}\n\n# Get the top ten classes with the highest frequencies\ntop_ten_classes = sorted(class_freq, key=class_freq.get, reverse=True)[:10]\n\n# Filter the confusion matrix to include only the top ten classes\ntop_ten_cm = cm[top_ten_classes][:, top_ten_classes]\n\n# Get the class names for the top ten classes\ntop_ten_class_names = [class_names[class_label] for class_label in top_ten_classes]\n\n# Plot the filtered confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(top_ten_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix for Top Ten Frequent Classes')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\n\n# Set tick labels\nplt.xticks(ticks=np.arange(10) + 0.5, labels=top_ten_class_names, rotation=45)\nplt.yticks(ticks=np.arange(10) + 0.5, labels=top_ten_class_names, rotation=0)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T01:19:10.934057Z","iopub.execute_input":"2024-05-17T01:19:10.934743Z","iopub.status.idle":"2024-05-17T01:19:11.527356Z","shell.execute_reply.started":"2024-05-17T01:19:10.934711Z","shell.execute_reply":"2024-05-17T01:19:11.526431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deployment Using Streamlit: [Link](https://www.kaggle.com/code/abdallahprogrammer/fruit-classification-using-cnn-deployment)","metadata":{}}]}